{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhqAM0Q2fUPlhlOUWACKP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SjSterling/Cosmology/blob/main/Degree_Requiremts_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Major requirements code"
      ],
      "metadata": {
        "id": "Vjq_2OqD6LSy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLVGrCYh6CMW"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from collections import deque\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import time\n",
        "\n",
        "cached_major_links = {}\n",
        "\n",
        "def find_majors_links(url):\n",
        "    # Check if the major links have already been cached for this URL\n",
        "    if url in cached_major_links:\n",
        "        print(\"Majors Tab Found on (Cached):\", url)\n",
        "        return cached_major_links[url]\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch URL: {url}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    majors_tab_links = soup.select('a[href=\"#majorstextcontainer\"]')\n",
        "\n",
        "    if majors_tab_links:\n",
        "        print(\"Majors Tab Found on:\", url)\n",
        "\n",
        "        # Find all links within the Majors tab\n",
        "        majors_links = soup.find(\"div\", {\"id\": \"majorstextcontainer\"}).find_all(\"a\", href=True)\n",
        "\n",
        "        if majors_links:\n",
        "            majors_link_urls = [urljoin(url, link[\"href\"]) for link in majors_links]\n",
        "\n",
        "            # Cache the major links for this URL\n",
        "            cached_major_links[url] = majors_link_urls\n",
        "\n",
        "            return majors_link_urls\n",
        "\n",
        "    return []\n",
        "\n",
        "import csv\n",
        "import os  # Add this import for file path operations\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def extract_tables(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch URL: {url}\")\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Define a list of keywords to search for in heading text\n",
        "    keywords = [\"Requirements\", \"Major in\", \"Track\"]\n",
        "\n",
        "    # Find all tables on the page\n",
        "    tables = soup.find_all(\"table\", class_=\"sc_courselist\")\n",
        "\n",
        "    for table in tables:\n",
        "        # Try to find a unique identifier for the table name\n",
        "        table_name = None\n",
        "\n",
        "        # Check if the table has an associated heading with a specific class or ID\n",
        "        heading = table.find_previous([\"h2\", \"h3\"])\n",
        "        if heading:\n",
        "            heading_text = heading.text.strip()\n",
        "            for keyword in keywords:\n",
        "                if keyword in heading_text:\n",
        "                    # Replace invalid characters in table_name so the file doesnt break bc its dumb\n",
        "                    table_name = heading_text.replace(\"/\", \"-\")  # Replace '/' with '-'\n",
        "                    break\n",
        "\n",
        "        if table_name:\n",
        "            print(\"Table found on:\", url)\n",
        "            print(\"Table Name:\", table_name)\n",
        "\n",
        "            folder_name = \"degree_requirements_knowledgebase\"\n",
        "            if not os.path.exists(folder_name):\n",
        "                os.makedirs(folder_name)\n",
        "\n",
        "            csv_filename = f\"{table_name}.csv\"\n",
        "            csv_filename = \"\".join([c for c in csv_filename if c.isalnum() or c in (' ', '-', '_')])\n",
        "\n",
        "\n",
        "            csv_filepath = os.path.join(folder_name, csv_filename)\n",
        "\n",
        "            with open(csv_filepath, \"w\", newline=\"\") as csvfile:\n",
        "                csv_writer = csv.writer(csvfile)\n",
        "                csv_writer.writerow([\"Code\", \"Title\", \"Hours\"])\n",
        "\n",
        "                # Extracting all the dumb contents of table\n",
        "                for row in table.find_all(\"tr\"):\n",
        "                    columns = row.find_all(\"td\")\n",
        "                    if len(columns) == 1:\n",
        "                        code = columns[0].text.strip()\n",
        "                        title = \"\"\n",
        "                        hours = \"\"\n",
        "                    elif len(columns) == 2:\n",
        "                        code = columns[0].text.strip()\n",
        "                        hours = columns[1].text.strip()\n",
        "                        title = \"\"\n",
        "                    elif len(columns) >= 3:\n",
        "                        code = columns[0].text.strip()\n",
        "                        title = columns[1].text.strip()\n",
        "                        hours = columns[2].text.strip()\n",
        "                    else:\n",
        "                        code = \"\"\n",
        "                        title = \"\"\n",
        "                        hours = \"\"\n",
        "\n",
        "                    csv_writer.writerow([code, title, hours])\n",
        "\n",
        "            print(f\"Table data saved to {csv_filepath}\")\n",
        "            print(\"---\")\n",
        "        else:\n",
        "            print(\"Skipping row with no columns.\")\n",
        "\n",
        "\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "\n",
        "start_url = \"http://catalog.valdosta.edu/undergraduate/academic-programs/\"\n",
        "\n",
        "queue = deque()\n",
        "queue.append(start_url)\n",
        "\n",
        "visited_urls = set()\n",
        "\n",
        "while queue:\n",
        "    current_url = queue.popleft()\n",
        "\n",
        "    if current_url in visited_urls:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Check if the current URL ends with \"/#header\", \"/#print-dialog\", or \"/#content\" and skip it if so\n",
        "        if current_url.endswith(\"/#header\") or current_url.endswith(\"/#print-dialog\") or current_url.endswith(\"/#content\"):\n",
        "            visited_urls.add(current_url)  # Mark it as visited to avoid retries\n",
        "            continue\n",
        "\n",
        "        majors_links = find_majors_links(current_url)\n",
        "\n",
        "        for majors_link in majors_links:\n",
        "            extract_tables(majors_link)\n",
        "\n",
        "        response = session.get(current_url, headers=headers, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "            for link in links:\n",
        "                full_url = urljoin(current_url, link[\"href\"])\n",
        "                if full_url.startswith(start_url):\n",
        "                    queue.append(full_url)\n",
        "\n",
        "        visited_urls.add(current_url)\n",
        "    except (requests.RequestException, ConnectionError) as e:\n",
        "        print(f\"Error fetching {current_url}: {e}\")\n",
        "        # delay so website doesnt block my ass\n",
        "        time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minor requirements code"
      ],
      "metadata": {
        "id": "DiLfogF_6Pha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from collections import deque\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import time\n",
        "import os\n",
        "\n",
        "cached_minor_links = {}\n",
        "\n",
        "def find_minor_links(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch URL: {url}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Find the tab with the \"Minors\" keyword in the text\n",
        "    tab_links = soup.find_all(\"a\", href=lambda href: href and \"#minorstextcontainer\" in href)\n",
        "\n",
        "    if tab_links:\n",
        "        print(\"Minors Tab Found on:\", url)\n",
        "\n",
        "        # Find all links within the \"Minors\" tab\n",
        "        tab_links = soup.find(\"div\", {\"id\": \"minorstextcontainer\"}).find_all(\"a\", href=True)\n",
        "\n",
        "        if tab_links:\n",
        "            tab_link_urls = [urljoin(url, link[\"href\"]) for link in tab_links]\n",
        "\n",
        "            return tab_link_urls\n",
        "\n",
        "    return []\n",
        "\n",
        "def extract_tables(url, folder_name):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch URL: {url}\")\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Define a list of keywords to search for in heading text\n",
        "    keywords = [\"Requirements\", \"Minor in\"]\n",
        "\n",
        "    # Find all tables on the page\n",
        "    tables = soup.find_all(\"table\", class_=\"sc_courselist\")\n",
        "\n",
        "    for table in tables:\n",
        "        # Try to find a unique identifier for the table name\n",
        "        table_name = None\n",
        "\n",
        "        # Check if the table has an associated heading with a specific class or ID\n",
        "        heading = table.find_previous([\"h2\", \"h3\"])\n",
        "        if heading:\n",
        "            heading_text = heading.text.strip()\n",
        "            # Check if any of the keywords are present in the heading text\n",
        "            for keyword in keywords:\n",
        "                if keyword in heading_text:\n",
        "                    # Replace invalid characters in table_name\n",
        "                    table_name = heading_text.replace(\"/\", \"-\")  # Replace '/' with '-'\n",
        "                    break\n",
        "\n",
        "        if table_name:\n",
        "            print(\"Table found on:\", url)\n",
        "            print(\"Table Name:\", table_name)\n",
        "\n",
        "            # Create the folder if it doesn't exist\n",
        "            if not os.path.exists(folder_name):\n",
        "                os.makedirs(folder_name)\n",
        "\n",
        "            # Create a valid CSV file name by removing invalid characters\n",
        "            csv_filename = f\"{table_name}.csv\"\n",
        "            csv_filename = \"\".join([c for c in csv_filename if c.isalnum() or c in (' ', '-', '_')])\n",
        "\n",
        "            # Create the full path to the CSV file inside the folder\n",
        "            csv_filepath = os.path.join(folder_name, csv_filename)\n",
        "\n",
        "            with open(csv_filepath, \"w\", newline=\"\") as csvfile:\n",
        "                csv_writer = csv.writer(csvfile)\n",
        "                csv_writer.writerow([\"Code\", \"Title\", \"Hours\"])\n",
        "\n",
        "                # Extract and write the contents of the current table to the CSV file\n",
        "                for row in table.find_all(\"tr\"):\n",
        "                    columns = row.find_all(\"td\")\n",
        "                    if len(columns) == 1:\n",
        "                        code = columns[0].text.strip()\n",
        "                        title = \"\"\n",
        "                        hours = \"\"\n",
        "                    elif len(columns) == 2:\n",
        "                        code = columns[0].text.strip()\n",
        "                        hours = columns[1].text.strip()\n",
        "                        title = \"\"\n",
        "                    elif len(columns) >= 3:\n",
        "                        code = columns[0].text.strip()\n",
        "                        title = columns[1].text.strip()\n",
        "                        hours = columns[2].text.strip()\n",
        "                    else:\n",
        "                        code = \"\"\n",
        "                        title = \"\"\n",
        "                        hours = \"\"\n",
        "\n",
        "                    csv_writer.writerow([code, title, hours])\n",
        "\n",
        "            print(f\"Table data saved to {csv_filepath}\")\n",
        "            print(\"---\")\n",
        "        else:\n",
        "            print(\"Skipping row with no columns.\")\n",
        "\n",
        "# Define the start URL\n",
        "start_url = \"http://catalog.valdosta.edu/undergraduate/academic-programs/\"\n",
        "\n",
        "# Create a session with retries\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "# Define headers\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Create a queue to store URLs to be visited\n",
        "queue = deque()\n",
        "queue.append(start_url)\n",
        "\n",
        "# Create a set to keep track of visited URLs\n",
        "visited_urls = set()\n",
        "\n",
        "while queue:\n",
        "    current_url = queue.popleft()\n",
        "\n",
        "    if current_url in visited_urls:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Check if the current URL ends with \"/#header\", \"/#print-dialog\", or \"/#content\" and skip it if so\n",
        "        if current_url.endswith(\"/#header\") or current_url.endswith(\"/#print-dialog\") or current_url.endswith(\"/#content\"):\n",
        "            visited_urls.add(current_url)  # Mark it as visited to avoid retries\n",
        "            continue\n",
        "\n",
        "        # Find and process minor links\n",
        "        minor_links = find_minor_links(current_url)\n",
        "\n",
        "        for minor_link in minor_links:\n",
        "            extract_tables(minor_link, \"minor_requirements_knowledgebase\")\n",
        "\n",
        "        # Fetch additional links to continue crawling\n",
        "        response = session.get(current_url, headers=headers, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            links = soup.find_all(\"a\", href=True)\n",
        "\n",
        "            for link in links:\n",
        "                full_url = urljoin(current_url, link[\"href\"])\n",
        "                if full_url.startswith(start_url):\n",
        "                    queue.append(full_url)\n",
        "\n",
        "        visited_urls.add(current_url)\n",
        "    except (requests.RequestException, ConnectionError) as e:\n",
        "        print(f\"Error fetching {current_url}: {e}\")\n",
        "        # Add a delay before retrying\n",
        "        time.sleep(5)\n"
      ],
      "metadata": {
        "id": "hfpETV0O6FXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}